{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Hypers --------\n",
      "- epochs: 50\n",
      "- learning rate: 0.5\n",
      "- hidden size: 100\n",
      "----------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'s_shift'",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8f818f02daed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0mmodel_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8f818f02daed>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, hidden_size, device)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mtrain_code_in_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_comment_in_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_word_size_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_word_size_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mval_code_in_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_comment_in_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_word_size_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_word_size_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/valid.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mtest_code_in_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_comment_in_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_word_size_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_word_size_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8f818f02daed>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mcode_in_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-01/28/828/shgao/SBT_encode.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0msbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSbt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0msbt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr2json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-01/28/828/shgao/SBT_encode.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0msbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSbt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0msbt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr2json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 's_shift'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from seq2seq import seq2seq\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import SBT_encode\n",
    "import re\n",
    "\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.loss = .0\n",
    "        self.count_data = 0\n",
    "        self.count_save = 0\n",
    "        self.count_chunk = 0\n",
    "        self.history = {}\n",
    "\n",
    "    def reset(self, epoch):\n",
    "        self.epoch = epoch\n",
    "        self.loss = .0\n",
    "        self.count_data = 0\n",
    "        self.count_save = 0\n",
    "        self.count_chunk = 0\n",
    "        self.history[epoch] = []\n",
    "\n",
    "    def __call__(self, loss, outputs):\n",
    "        loss_ = loss.cpu().detach().numpy()\n",
    "        outputs_ = outputs.cpu().detach().numpy().squeeze()\n",
    "        chunk_size = outputs_.shape[0]\n",
    "        self.loss += loss_ * chunk_size\n",
    "        self.count_data += chunk_size\n",
    "        self.count_chunk += 1\n",
    "\n",
    "    def avg_loss(self):\n",
    "        return self.loss / self.count_data\n",
    "\n",
    "    def save(self, train_loss, val_loss):\n",
    "        self.count_save += 1\n",
    "        self.history[self.epoch].append((train_loss, val_loss))\n",
    "\n",
    "\n",
    "def preprocessing(file_name):\n",
    "    # load data\n",
    "    with open(file_name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    comment = []\n",
    "    code = []\n",
    "    for i in range(len(data)):\n",
    "        temp_comment, temp_code = data[i]\n",
    "        comment.append(temp_comment)\n",
    "        code.append(temp_code)\n",
    "    \n",
    "    set_word = set()\n",
    "    pattern = r',|\\.|/|;|\\'|`|\\[|\\]|<|>|\\?|:|\"|\\{|\\}|\\~|!|@|#|\\$|%|\\^|&|\\(|\\)|-|=|\\_|\\+|，|。|、|；|‘|’|【|】|·|！| |…|（|）'\n",
    "    for i in range(len(comment)):\n",
    "        temp_list = re.split(pattern, comment[i])\n",
    "        for x in temp_list:\n",
    "            set_word.add(x)\n",
    "    commment_wordlist = list(set_word)\n",
    "    comment_dict = dict(zip(commment_wordlist, range(len(commment_wordlist))))\n",
    "    \n",
    "    #print(comment_dict)\n",
    "\n",
    "    encoder = SBT_encode.Encoder()\n",
    "\n",
    "    code_in_num = []\n",
    "    comment_in_num = []\n",
    "\n",
    "    for i in range(len(code)):\n",
    "        code_in_num.append(encoder.encode(code[i]))\n",
    "\n",
    "    for i in range(len(comment)):\n",
    "        split_list = re.split(pattern, comment[i])\n",
    "        temp_list = []\n",
    "        for x in split_list:\n",
    "            temp_list.append(x)\n",
    "        comment_in_num.append(temp_list)\n",
    "\n",
    "    max_len_code = max([len(code_in_num[i]) for i in range(len(code_in_num))])\n",
    "    max_len_comment = max([len(comment_in_num[i]) for i in range(len(comment_in_num))])\n",
    "\n",
    "    for i in range(len(code_in_num)):\n",
    "        while len(code_in_num[i]) < max_len_code:\n",
    "            code_in_num[i].append(0)\n",
    "\n",
    "    for i in range(len(comment_in_num)):\n",
    "        while len(comment_in_num[i]) < max_len_comment:\n",
    "            comment_in_num[i].append(0)\n",
    "\n",
    "    return code_in_num, comment_in_num, max_len_code, max_len_comment\n",
    "\n",
    "\n",
    "def build_model(word_size_encoder, word_size_decoder, emb_dim=10, hidden_size=100, learning_rate=0.1, device=None):\n",
    "    model = seq2seq(word_size_encoder, emb_dim, hidden_size, word_size_decoder)\n",
    "    # run on the gpu or cpu\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataloaders,\n",
    "                num_epochs=1, best_loss=10,\n",
    "                evaluate=Evaluation(), device=None):\n",
    "    # init timer\n",
    "    since = time.time()\n",
    "    start_epoch = evaluate.epoch\n",
    "    step = 500\n",
    "    # if istest: step = 10\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        print('\\nEpoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        ## reset evaluator in a new epoch\n",
    "        evaluate.reset(epoch)\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(dataloaders['train']):\n",
    "\n",
    "            # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # regular stuff\n",
    "            outputs = model(inputs, targets)\n",
    "            # squeeze the unnecessary batchsize dim\n",
    "            loss = criterion(outputs, targets.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # evaluation\n",
    "            evaluate(loss, outputs)\n",
    "\n",
    "            # validate every n chunks\n",
    "            if i % step == 0:\n",
    "                train_loss = evaluate.avg_loss()\n",
    "                # validate first\n",
    "                val_loss = validate_model(model, criterion,\n",
    "                                          dataloaders['val'],\n",
    "                                          device=device)\n",
    "\n",
    "                # update best loss\n",
    "                is_best = val_loss < best_loss\n",
    "                best_loss = min(val_loss, best_loss)\n",
    "\n",
    "                # verbose\n",
    "                print('[%i] '\n",
    "                      'train-loss: %.4f '\n",
    "                      'val-loss: %.4f '\n",
    "                      '' % (evaluate.count_save,\n",
    "                            train_loss,\n",
    "                            val_loss))\n",
    "\n",
    "                # save for plot\n",
    "                evaluate.save(train_loss, val_loss)\n",
    "                save_checkpoint({'model': model.state_dict(),\n",
    "                                 'optimizer': optimizer.state_dict(),\n",
    "                                 'best_loss': best_loss,\n",
    "                                 'history': evaluate}, is_best)\n",
    "\n",
    "            # if istest:\n",
    "            #     if i == 100: break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('\\nTraining complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "# could also be use to test\n",
    "def validate_model(model, criterion, loader, device=None, verbose=False):\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "    evaluate = Evaluation()\n",
    "    step = 50\n",
    "    # if istest: step = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, (inputs, targets) in enumerate(loader):\n",
    "            # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.squeeze())\n",
    "            evaluate(loss, outputs)\n",
    "\n",
    "            if verbose:\n",
    "                if j % step == 0:\n",
    "                    print('[%i] val-loss: %.4f' % (j, evaluate.avg_loss()))\n",
    "\n",
    "            # if istest:\n",
    "            #     if j == 2: break\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    return evaluate.avg_loss()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best):\n",
    "    filename = 'checkpoint' + str(model_num) + '.pth.tar'\n",
    "    bestname = 'model_best' + str(model_num) + '.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, bestname)\n",
    "\n",
    "\n",
    "def check_cuda():\n",
    "    # Check if your system supports CUDA\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # Setup GPU optimization if CUDA is supported\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    else:  # Otherwise, train on the CPU\n",
    "        device = torch.device(\"cpu\")\n",
    "        extras = False\n",
    "    return use_cuda, device, extras\n",
    "\n",
    "\n",
    "def main(learning_rate=0.01, hidden_size=100, device=None):\n",
    "    # hyperparameters\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.5\n",
    "    # hidden_size = 100\n",
    "\n",
    "    print('------- Hypers --------\\n'\n",
    "          '- epochs: %i\\n'\n",
    "          '- learning rate: %g\\n'\n",
    "          '- hidden size: %i\\n'\n",
    "          '----------------'\n",
    "          '' % (num_epochs, learning_rate, hidden_size))\n",
    "\n",
    "    train_code_in_num, train_comment_in_num, train_word_size_encoder, train_word_size_decoder = preprocessing('data/train.pkl')\n",
    "    val_code_in_num, val_comment_in_num, val_word_size_encoder, val_word_size_decoder = preprocessing('data/valid.pkl')\n",
    "    test_code_in_num, test_comment_in_num, test_word_size_encoder, test_word_size_decoder = preprocessing('data/test.pkl')\n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = (train_code_in_num, train_comment_in_num)\n",
    "    dataloaders['val'] = (val_code_in_num, val_comment_in_num)\n",
    "    dataloaders['test'] = (test_code_in_num, test_comment_in_num)\n",
    "\n",
    "    # save loader and encoder for later use\n",
    "    # torch.save({'loaders': dataloaders,\n",
    "    #             'encoder': encoder,\n",
    "    #             'hidden_size': hidden_size},\n",
    "    #            'init' + str(model_num) + '.pth.tar')\n",
    "\n",
    "    model, criterion, optimizer = build_model(train_word_size_encoder, train_word_size_decoder,\n",
    "                                              emb_dim=10, hidden_size=100, learning_rate=0.1, device=None)\n",
    "    evaluate = Evaluation()\n",
    "\n",
    "    train_model(model, criterion, optimizer, dataloaders,\n",
    "                num_epochs=num_epochs, evaluate=evaluate,\n",
    "                best_loss=10, device=device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_num = 0\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
